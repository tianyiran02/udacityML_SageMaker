{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析网络应用\n",
    "\n",
    "_机器学习工程师纳米学位课程 | 部署_\n",
    "\n",
    "---\n",
    "\n",
    "在此 notebook 中，我们将使用 Amazon SageMaker 服务构建一个随机树模型来预测影评的情感。此外，我们会将该模型部署到端点上，并构建一个与模型的部署端点交互的简单网络应用。\n",
    "\n",
    "## 一般步骤\n",
    "\n",
    "通常，在 notebook 实例中使用 SageMaker 时，你需要完成以下步骤。当然，并非每个项目都要完成每一步。此外，有很多步骤有很大的变化余地，你将在这些课程中发现这一点。\n",
    "\n",
    "1. 下载或检索数据。\n",
    "2. 处理/准备数据。\n",
    "3. 将处理的数据上传到 S3。\n",
    "4. 训练所选的模型。\n",
    "5. 测试训练的模型（通常使用批转换作业）。\n",
    "6. 部署训练的模型。\n",
    "7. 使用部署的模型。\n",
    "\n",
    "在此 notebook 中，我们将完成上述每一步。你会发现最后一步（使用部署的模型）很有挑战性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 1 步：下载数据\n",
    "\n",
    "我们要使用的数据集很受自然语言处理领域的研究者欢迎，通常称为 [IMDB 数据集](http://ai.stanford.edu/~amaas/data/sentiment/)。其中包含网站 [imdb.com](http://www.imdb.com/) 上的影评，每条影评都标有 '**pos**itive'，表示评论者喜欢影片，否则标有 '**neg**ative'。\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011.\n",
    "\n",
    "我们先通过 Jupyter Notebook 功能下载和提取该数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2020-03-26 12:45:09--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  55.1MB/s    in 1.5s    \n",
      "\n",
      "2020-03-26 12:45:10 (55.1 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 2 步：准备和处理数据\n",
    "\n",
    "我们下载的文件拆分成了各种文件，每个都包含一条影评。我们需要将这些文件合并成两个文件，一个用于训练，一个用于测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yakitate! Ja-pan (translated as Fresh Baked! Japanese Bread) is the story of a young man named Azuma Kazuma and his journey to make the perfect Japanese Bread or Ja-pan, for Japan, and for the Japanese, that will be recognized the the whole world.<br /><br />Of course, that's just on the outside. In reality, Yakitate! Ja-pan isn't really about the bread, but the reaction that come after eating the bread, and the pun that comes with the reaction. The series is lovable because of these puns. From popular anime titles like Naruto, Detective Conan, and Dragon Ball to blockbuster movies like The Matrix and Lord of the Rings. It's all there.<br /><br />So what makes this title different from other titles of the same genre like Cooking Master Boy or Mr. Ajikko? Well, unlike the others who use cooking for world domination, Yakitate! Ja-pan is purely comedy. Sure, there are times that the story turns to drama, or even murder, but the comedic atmosphere makes you laugh at them. You'll be laughing at their own view of heaven. Just watch it.<br /><br />Just remember that this is also fiction, although some of the bread made here are based on real bread, eating the home made Japan #2 won't turn you to a Super Saiyan or turn your body to rubber.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据\n",
    "\n",
    "合并并准备好训练和测试数据集后，我们需要将原始数据处理成机器学习算法能够使用的格式。首先，删除所有的 HTML 格式标记以及非字母数字字符。方法很简单，使用 Python 的正则表达式模块即可。稍后我们会讲解为何执行这个很简单的预处理步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def review_to_words(review):\n",
    "    words = REPLACE_NO_SPACE.sub(\"\", review.lower())\n",
    "    words = REPLACE_WITH_SPACE.sub(\" \", words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yakitate ja pan translated as fresh baked japanese bread is the story of a young man named azuma kazuma and his journey to make the perfect japanese bread or ja pan for japan and for the japanese that will be recognized the the whole world of course thats just on the outside in reality yakitate ja pan isnt really about the bread but the reaction that come after eating the bread and the pun that comes with the reaction the series is lovable because of these puns from popular anime titles like naruto detective conan and dragon ball to blockbuster movies like the matrix and lord of the rings its all there so what makes this title different from other titles of the same genre like cooking master boy or mr ajikko well unlike the others who use cooking for world domination yakitate ja pan is purely comedy sure there are times that the story turns to drama or even murder but the comedic atmosphere makes you laugh at them youll be laughing at their own view of heaven just watch it just remember that this is also fiction although some of the bread made here are based on real bread eating the home made japan #2 wont turn you to a super saiyan or turn your body to rubber'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_web_app\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取词袋特征\n",
    "\n",
    "对于我们要实现的模型，我们并不直接使用影评，而是将每条影评转换成词袋特征表示法。注意，我们只能访问训练集，所以转换器只能使用训练集创建表示结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to cache file: bow_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 3 步：将数据上传到 S3\n",
    "\n",
    "创建了训练（和测试）数据的特征表示结果后，我们将开始设置和使用 SageMaker 提供的 XGBoost 分类器。\n",
    "\n",
    "### 写入数据集\n",
    "\n",
    "我们将使用的 XGBoost 分类器要求我们将数据集写入文件中并将文件上传到 Amazon S3。我们首先将训练数据集拆分成两部分，分别是训练集和验证集。然后，将这些数据集写入本地文件中，并将文件上传到 S3。此外，我们将测试集写入文件中并将该文件上传到 S3。这样才能使用 SageMaker 批转换功能测试拟合后的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Earlier we shuffled the training dataset so to make things simple we can just assign\n",
    "# the first 10 000 reviews to the validation set and use the remaining reviews for training.\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker 中的 XGBoost 算法的参考文档要求训练集和验证集不包含标题或索引，并且每个样本的标签在前面。\n",
    "\n",
    "要详细了解此算法以及其他算法，请参阅 [Amazon SageMaker 开发人员文档](https://docs.aws.amazon.com/sagemaker/latest/dg/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make sure that the local directory in which we'd like to store the training and validation csv files exists.\n",
    "data_dir = '../data/sentiment_web_app'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a bit of memory we can set text_X, train_X, val_X, train_y and val_y to None.\n",
    "\n",
    "test_X = train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练/验证文件上传到 S3\n",
    "\n",
    "Amazon S3 服务允许我们存储文件，内置训练模型（例如我们将使用的 XGBoost 模型）和自定义模型（例如我们稍后将查看的模型）可以访问这些文件。\n",
    "\n",
    "对于此任务以及将使用 SageMaker 完成的大多数其他任务，我们可以使用两种方法。一种是使用 SageMaker 的低阶方法，低阶方法要求我们知道在 SageMaker 环境中出现的每个对象。第二种是使用高阶方法，SageMaker 会代替我们做出一些选择。低阶方法的好处是给用户带来了很高的灵活性，而高阶方法使开发速度快多了。对我们来说，我们将使用高阶方法，但是也可以使用低阶方法。\n",
    "\n",
    "方法 `upload_data()` 是代表当前 SageMaker 会话的对象的成员。该方法会将数据上传到默认存储桶（如果不存在的话，将会创建），并放入由 key_prefix 变量指定的路径下。上传数据文件后，你可以转到 S3 控制台并看看文件上传到哪了。\n",
    "\n",
    "要查看其他资源，请参阅 [SageMaker API 文档](http://sagemaker.readthedocs.io/en/latest/)以及 __[SageMaker 开发人员指南](https://docs.aws.amazon.com/sagemaker/latest/dg/)__。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = 'sentiment-web-app'\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 4 步：创建 XGBoost 模型\n",
    "\n",
    "上传数据后，下面开始创建 XGBoost 模型。首先需要进行设置。此刻有必要讨论下模型在 SageMaker 中的含义。简单来说，可以将模型看作由 SageMaker 生态系统中的三个不同对象组成，它们相互交互。\n",
    "\n",
    "- 模型工件\n",
    "- 训练代码（容器）\n",
    "- 推理代码（容器）\n",
    "\n",
    "你可以将模型工件看作实际模型本身。例如，在构建神经网络时，可以将模型工件看作各个层级的权重。在我们的示例中，XGBoost 模型的模型工件是在训练过程中创建的实际树。\n",
    "\n",
    "训练代码和推理代码然后将用来操纵训练工件。更准确地说，训练代码使用提供的训练数据并创建模型工件，而推理代码使用模型工件对新数据做出预测。\n",
    "\n",
    "SageMaker 使用 Docker 容器运行训练和推理代码。暂时将容器看作一种代码打包方式，使依赖项不存在问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Our current execution role is required when creating the model as the training\n",
    "# and inference code will need to access the model artifacts.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='0.90-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    }
   ],
   "source": [
    "# We need to retrieve the location of the container which is provided by Amazon for using XGBoost.\n",
    "# As a matter of convenience, the training and inference code both use the same container.\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a SageMaker estimator object for our model.\n",
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "# And then set the algorithm specific parameters.\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合 XGBoost 模型\n",
    "\n",
    "设置好模型后，我们只需附加训练集和验证集，然后要求 SageMaker 设置计算过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 12:52:13 Starting - Starting the training job...\n",
      "2020-03-26 12:52:16 Starting - Launching requested ML instances.........\n",
      "2020-03-26 12:53:46 Starting - Preparing the instances for training...\n",
      "2020-03-26 12:54:44 Downloading - Downloading input data\n",
      "2020-03-26 12:54:44 Training - Downloading the training image...\n",
      "2020-03-26 12:55:03 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-03-26:12:55:04:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-03-26:12:55:04:INFO] File size need to be processed in the node: 238.5mb. Available memory size in the node: 8491.97mb\u001b[0m\n",
      "\u001b[34m[2020-03-26:12:55:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[12:55:04] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[12:55:05] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-03-26:12:55:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[12:55:05] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[12:55:07] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[12:55:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.308867#011validation-error:0.3066\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[12:55:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.2922#011validation-error:0.2918\u001b[0m\n",
      "\u001b[34m[12:55:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.2792#011validation-error:0.2839\u001b[0m\n",
      "\u001b[34m[12:55:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.281267#011validation-error:0.2862\u001b[0m\n",
      "\u001b[34m[12:55:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.275267#011validation-error:0.2782\u001b[0m\n",
      "\u001b[34m[12:55:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.253667#011validation-error:0.2616\u001b[0m\n",
      "\u001b[34m[12:55:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.2466#011validation-error:0.2565\u001b[0m\n",
      "\u001b[34m[12:55:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.2414#011validation-error:0.2512\u001b[0m\n",
      "\u001b[34m[12:55:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.233267#011validation-error:0.2464\u001b[0m\n",
      "\u001b[34m[12:55:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.2264#011validation-error:0.2407\u001b[0m\n",
      "\u001b[34m[12:55:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.2192#011validation-error:0.2355\u001b[0m\n",
      "\u001b[34m[12:55:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.2156#011validation-error:0.2303\u001b[0m\n",
      "\u001b[34m[12:55:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.211733#011validation-error:0.2258\u001b[0m\n",
      "\u001b[34m[12:55:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.206133#011validation-error:0.2233\u001b[0m\n",
      "\u001b[34m[12:55:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.2028#011validation-error:0.219\u001b[0m\n",
      "\u001b[34m[12:55:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.200133#011validation-error:0.2167\u001b[0m\n",
      "\u001b[34m[12:55:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.199067#011validation-error:0.217\u001b[0m\n",
      "\u001b[34m[12:55:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.195333#011validation-error:0.2148\u001b[0m\n",
      "\u001b[34m[12:55:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.1932#011validation-error:0.2135\u001b[0m\n",
      "\u001b[34m[12:55:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.1894#011validation-error:0.2095\u001b[0m\n",
      "\u001b[34m[12:55:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.186467#011validation-error:0.2071\u001b[0m\n",
      "\u001b[34m[12:55:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.183733#011validation-error:0.2067\u001b[0m\n",
      "\u001b[34m[12:55:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.182467#011validation-error:0.2061\u001b[0m\n",
      "\u001b[34m[12:55:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.180333#011validation-error:0.205\u001b[0m\n",
      "\u001b[34m[12:55:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.1762#011validation-error:0.2036\u001b[0m\n",
      "\u001b[34m[12:55:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.174533#011validation-error:0.2035\u001b[0m\n",
      "\u001b[34m[12:55:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.173733#011validation-error:0.2038\u001b[0m\n",
      "\u001b[34m[12:55:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.172#011validation-error:0.2029\u001b[0m\n",
      "\u001b[34m[12:55:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.170533#011validation-error:0.1998\u001b[0m\n",
      "\u001b[34m[12:55:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.168467#011validation-error:0.197\u001b[0m\n",
      "\u001b[34m[12:55:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.167533#011validation-error:0.1947\u001b[0m\n",
      "\u001b[34m[12:55:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.165467#011validation-error:0.1935\u001b[0m\n",
      "\u001b[34m[12:55:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.163933#011validation-error:0.1926\u001b[0m\n",
      "\u001b[34m[12:55:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.1618#011validation-error:0.1902\u001b[0m\n",
      "\u001b[34m[12:55:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.16#011validation-error:0.1887\u001b[0m\n",
      "\u001b[34m[12:55:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.160133#011validation-error:0.1873\u001b[0m\n",
      "\u001b[34m[12:55:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.157933#011validation-error:0.1873\u001b[0m\n",
      "\u001b[34m[12:55:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.155933#011validation-error:0.1872\u001b[0m\n",
      "\u001b[34m[12:55:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.154133#011validation-error:0.1866\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[12:56:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.1378#011validation-error:0.1774\u001b[0m\n",
      "\u001b[34m[12:56:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.1372#011validation-error:0.1769\u001b[0m\n",
      "\u001b[34m[12:56:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.136467#011validation-error:0.1756\u001b[0m\n",
      "\u001b[34m[12:56:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.1336#011validation-error:0.1744\u001b[0m\n",
      "\u001b[34m[12:56:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.133067#011validation-error:0.1735\u001b[0m\n",
      "\u001b[34m[12:56:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.1324#011validation-error:0.1724\u001b[0m\n",
      "\u001b[34m[12:56:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.1316#011validation-error:0.1715\u001b[0m\n",
      "\u001b[34m[12:56:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.131933#011validation-error:0.1715\u001b[0m\n",
      "\u001b[34m[12:56:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.130733#011validation-error:0.1701\u001b[0m\n",
      "\u001b[34m[12:56:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.129533#011validation-error:0.169\u001b[0m\n",
      "\u001b[34m[12:56:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.128333#011validation-error:0.1712\u001b[0m\n",
      "\u001b[34m[12:56:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.127467#011validation-error:0.1698\u001b[0m\n",
      "\u001b[34m[12:56:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.126133#011validation-error:0.1699\u001b[0m\n",
      "\u001b[34m[12:56:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.126#011validation-error:0.1689\u001b[0m\n",
      "\u001b[34m[12:56:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.125533#011validation-error:0.1673\u001b[0m\n",
      "\u001b[34m[12:56:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.1252#011validation-error:0.1669\u001b[0m\n",
      "\u001b[34m[12:56:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.123333#011validation-error:0.1662\u001b[0m\n",
      "\u001b[34m[12:56:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.123133#011validation-error:0.1658\u001b[0m\n",
      "\u001b[34m[12:56:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.122867#011validation-error:0.1651\u001b[0m\n",
      "\u001b[34m[12:56:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.121467#011validation-error:0.1665\u001b[0m\n",
      "\u001b[34m[12:56:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.120533#011validation-error:0.1663\u001b[0m\n",
      "\u001b[34m[12:56:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.119867#011validation-error:0.1663\u001b[0m\n",
      "\u001b[34m[12:56:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.1192#011validation-error:0.1665\u001b[0m\n",
      "\u001b[34m[12:56:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.118267#011validation-error:0.165\u001b[0m\n",
      "\u001b[34m[12:56:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.1178#011validation-error:0.1645\u001b[0m\n",
      "\u001b[34m[12:56:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.116733#011validation-error:0.1644\u001b[0m\n",
      "\u001b[34m[12:56:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.1158#011validation-error:0.1647\u001b[0m\n",
      "\u001b[34m[12:56:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.115467#011validation-error:0.1651\u001b[0m\n",
      "\u001b[34m[12:56:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.114667#011validation-error:0.163\u001b[0m\n",
      "\u001b[34m[12:56:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.114533#011validation-error:0.1632\u001b[0m\n",
      "\u001b[34m[12:56:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.113133#011validation-error:0.1628\u001b[0m\n",
      "\u001b[34m[12:56:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.111867#011validation-error:0.1642\u001b[0m\n",
      "\u001b[34m[12:56:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.111267#011validation-error:0.1646\u001b[0m\n",
      "\u001b[34m[12:56:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.111733#011validation-error:0.1639\u001b[0m\n",
      "\u001b[34m[12:56:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.1112#011validation-error:0.1623\u001b[0m\n",
      "\u001b[34m[12:57:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.110667#011validation-error:0.161\u001b[0m\n",
      "\u001b[34m[12:57:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.109933#011validation-error:0.1617\u001b[0m\n",
      "\u001b[34m[12:57:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.109933#011validation-error:0.161\u001b[0m\n",
      "\u001b[34m[12:57:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.109067#011validation-error:0.1596\u001b[0m\n",
      "\u001b[34m[12:57:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.1086#011validation-error:0.16\u001b[0m\n",
      "\u001b[34m[12:57:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.107933#011validation-error:0.1582\u001b[0m\n",
      "\u001b[34m[12:57:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.1082#011validation-error:0.1581\u001b[0m\n",
      "\u001b[34m[12:57:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.107733#011validation-error:0.1582\u001b[0m\n",
      "\u001b[34m[12:57:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.1068#011validation-error:0.1582\u001b[0m\n",
      "\u001b[34m[12:57:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.1066#011validation-error:0.1584\u001b[0m\n",
      "\u001b[34m[12:57:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.1064#011validation-error:0.1586\u001b[0m\n",
      "\u001b[34m[12:57:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.106267#011validation-error:0.1574\u001b[0m\n",
      "\u001b[34m[12:57:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.105733#011validation-error:0.1572\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[12:57:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.104667#011validation-error:0.156\u001b[0m\n",
      "\u001b[34m[12:57:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.104867#011validation-error:0.1558\u001b[0m\n",
      "\u001b[34m[12:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.104533#011validation-error:0.1551\u001b[0m\n",
      "\u001b[34m[12:57:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.105067#011validation-error:0.1547\u001b[0m\n",
      "\u001b[34m[12:57:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.104933#011validation-error:0.1554\u001b[0m\n",
      "\u001b[34m[12:57:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.104267#011validation-error:0.1549\u001b[0m\n",
      "\u001b[34m[12:57:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.103933#011validation-error:0.1548\u001b[0m\n",
      "\u001b[34m[12:57:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[106]#011train-error:0.103333#011validation-error:0.1548\u001b[0m\n",
      "\u001b[34m[12:57:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[107]#011train-error:0.1032#011validation-error:0.1546\u001b[0m\n",
      "\u001b[34m[12:57:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[108]#011train-error:0.1026#011validation-error:0.1545\u001b[0m\n",
      "\u001b[34m[12:57:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[109]#011train-error:0.101867#011validation-error:0.1532\u001b[0m\n",
      "\u001b[34m[12:57:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[110]#011train-error:0.1008#011validation-error:0.154\u001b[0m\n",
      "\u001b[34m[12:57:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[111]#011train-error:0.100733#011validation-error:0.1537\u001b[0m\n",
      "\u001b[34m[12:57:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[112]#011train-error:0.100267#011validation-error:0.1529\u001b[0m\n",
      "\u001b[34m[12:57:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[113]#011train-error:0.099#011validation-error:0.1534\u001b[0m\n",
      "\u001b[34m[12:57:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[114]#011train-error:0.098467#011validation-error:0.1531\u001b[0m\n",
      "\u001b[34m[12:57:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[115]#011train-error:0.097533#011validation-error:0.1533\u001b[0m\n",
      "\u001b[34m[12:57:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[116]#011train-error:0.097933#011validation-error:0.1533\u001b[0m\n",
      "\u001b[34m[12:57:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[117]#011train-error:0.097733#011validation-error:0.1539\u001b[0m\n",
      "\u001b[34m[12:57:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[118]#011train-error:0.097067#011validation-error:0.154\u001b[0m\n",
      "\u001b[34m[12:57:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[119]#011train-error:0.0964#011validation-error:0.1542\u001b[0m\n",
      "\u001b[34m[12:57:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[120]#011train-error:0.0964#011validation-error:0.154\u001b[0m\n",
      "\u001b[34m[12:57:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[121]#011train-error:0.096533#011validation-error:0.1532\u001b[0m\n",
      "\u001b[34m[12:57:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[122]#011train-error:0.096667#011validation-error:0.153\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[112]#011train-error:0.100267#011validation-error:0.1529\n",
      "\u001b[0m\n",
      "\n",
      "2020-03-26 12:57:57 Uploading - Uploading generated training model\n",
      "2020-03-26 12:57:57 Completed - Training job completed\n",
      "Training seconds: 212\n",
      "Billable seconds: 212\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 5 步：测试模型\n",
    "\n",
    "拟合 XGBoost 模型后，下面看看模型的效果如何。我们将使用 SageMaker 的批转换功能。通过批转换可以轻松地对大型数据集进行推理，因为它并非实时执行。我们并不需要立即使用模型的结果，可以对大量样本进行推理。示例行业应用包括月末报告。这种推理方法的另一个用处是可以对整个测试集进行推理。\n",
    "\n",
    "为了执行批转换，我们首先需要根据训练过的 estimator 对象创建一个 transformer 对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: xgboost-2020-03-26-12-52-13-753\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来执行转换作业。我们需要指定要发送的数据的类型，使 SageMaker 能够在后台正确地序列化数据。我们将向模型提供 csv 数据，所以指定为 `text/csv`。此外，如果我们提供的测试数据太大，无法一次性处理完，我们需要指定文件的拆分方式。因为数据集中的每行就是一个条目，所以我们将按照每行拆分输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前转换作业已经在运行，不过是在后台运行。因为我们要等待转换作业运行完毕，所以可以使用 `wait()` 方法查看运行进度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-03-26 13:12:53 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-03-26 13:12:53 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-03-26 13:12:53 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-03-26 13:12:53 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-03-26 13:12:53 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2020-03-26 13:12:53 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:12:53:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2020-03-26 13:12:53 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:12:53:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:12:53:INFO] Model loaded successfully for worker : 42\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:12:53:INFO] Model loaded successfully for worker : 43\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:16:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:16:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:15:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:16:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:16:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2020-03-26T13:13:12.771:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:18:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:18:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:19:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:19:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:20:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:20:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:21:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:21:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:21:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:20:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:21:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:21:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:21:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:21:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:23:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:26:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:26:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:28:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:28:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:28:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:28:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-03-26:13:13:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-26:13:13:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-26:13:13:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在转换作业已经执行并且结果（每条影评的预测情感）已经保存到 S3 上。因为我们要在本地分析文件，所以通过一个 notebook 功能将文件复制到 `data_dir`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/369.0 KiB (2.2 MiB/s) with 1 file(s) remaining\r",
      "Completed 369.0 KiB/369.0 KiB (3.1 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-1-270372225889/xgboost-2020-03-26-13-09-34-539/test.csv.out to ../data/sentiment_web_app/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步是读入模型的输出，将输出转换成可用的格式，我们希望情感为 `1`（正面）或 `0`（负面），然后与真实标签进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84936"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 6 步：部署模型\n",
    "\n",
    "构建并拟合模型后，SageMaker 将存储生成的模型工件，我们可以使用这些工件部署端点（推理代码）。请打开 SageMaker 控制台，你将看到 SageMaker 创建了一个模型，并且能看到模型工件在 S3 上的存储位置链接。\n",
    "\n",
    "部署端点与训练模型很相似，不过有几个重要区别。首先是部署的模型不会更改模型工件，所以当你向它发送各种测试样本时，模型不会更改。另一个区别是因为我们没有执行固定计算（训练步骤或批转换时会执行固定计算），所以启动的计算实例会一直运行，直到我们停止它。这一点很重要，因为如果忘记关闭了，将会一直计费。\n",
    "\n",
    "换句话说，**如果不再使用部署的端点，请关闭！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （再次）测试模型\n",
    "\n",
    "部署端点后，我们可以向其发送测试数据并获得推理结果。我们之前已经使用 SageMaker 的批转换功能测试模型，但是这次将使用新部署的端点再次测试模型，以便检查端点是否能正常运行，并了解端点的运行效果。\n",
    "\n",
    "在使用创建的端点时，需要注意的是，每次调用能够发送的信息量是有限的，所以需要将测试数据分成一份份，然后一份份地发送。此外，我们需要序列化数据，然后才能发送给端点，以确保数据能正确传输。幸运的是，只要提供了数据格式，SageMaker 就能为我们执行序列化步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# We need to tell the endpoint what format the data we are sending is in so that SageMaker can perform the serialization.\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "    \n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None).values\n",
    "\n",
    "predictions = predict(test_X)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，看看模型的准确率有多高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84936"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果应该与之前使用批转换作业测试模型后的结果一致。\n",
    "\n",
    "### 清理数据\n",
    "\n",
    "确认部署的模型行为符合预期后，我们将关闭端点。注意，端点运行时间越久，费用就越高，因为在我们的简单网络应用能够使用端点之前，还有一些准备工作要做，所以我们将关闭一切。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 7 步：使用模型\n",
    "\n",
    "我之前多次提到，我们的目标是部署模型并用很简单的网络应用访问该模型。这款网络应用能够接受用户提交的数据（影评），将数据发送给端点（模型），然后显示结果。\n",
    "\n",
    "但是，还有一个小问题。目前，我们只能使用 SageMaker API 访问端点并向其发送数据。我们可以获得向模型端点发送数据的实际 URL，但是如果我们自己向其发送数据，将不会获得任何结果。这是因为 SageMaker 创建的端点要求访问端点的实体具有相应的权限。所以我们需要向 AWS 验证网络应用的身份。\n",
    "\n",
    "向 AWS 验证网站的身份似乎超出了这节课的范畴，所以我们将采用另一种方法。我们将创建新的端点，该端点不需要验证身份，并且充当 SageMaker 端点的代理。\n",
    "\n",
    "还有一个约束条件，我们将避免在网络应用本身里执行任何数据处理步骤。在构建和测试模型时，原始数据是影评，然后我们删除了 HTML 格式标记和标点，并创建了词袋嵌入，然后将生成的向量发送给模型。所有这些处理步骤还要应用到用户输入上。\n",
    "\n",
    "幸运的是，我们可以使用 Amazon 的 Lambda 服务在后台完成所有这些处理步骤。\n",
    "\n",
    "<img src=\"Web App Diagram.svg\">\n",
    "\n",
    "上面的示意图解释了各种服务是如何协同工作的。最右边的是模型，我们在上面训练了模型，并且使用 SageMaker 部署了模型。最左侧的是网络应用，应用将收集用户的影评，将其发送给端点，并获得正面或负面情感预测。\n",
    "\n",
    "中间的部分比较关键。我们将创建一个 Lambda 函数，你可以将其看做一个简单的 Python 函数，每次发生特定的事件时，该 Python 函数就会被执行。该 Python 函数将对用户提交的输入执行必要的数据处理步骤。此外，该函数有权向 SageMaker 端点发送数据及从中接收数据。\n",
    "\n",
    "最后，我们执行 Lambda 函数所使用的是新的端点，我们将使用 API Gateway 创建该端点。端点获得数据后，会将数据传递给 Lambda 函数，并返回 Lambda 函数返回的结果。它充当了网络应用与 Lambda 函数之间的通信接口。\n",
    "### 处理单个影评\n",
    "\n",
    "假设用户提交了如下所示的字符串影评："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"Nothing but a disgusting materialistic pageant of glistening abed remote control greed zombies, totally devoid of any heart or heat. A romantic comedy that has zero romantic chemestry and zero laughs!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何将此字符串变成模型要求的词袋特征向量？\n",
    "\n",
    "在此 notebook 的开头，我们首先使用 `review_to_words` 方法删除所有不需要的字符。我们故意采用了很简单的方式，这是因为我们需要将此方法复制到最终的 Lambda 函数（稍后详细讲解），所以该方法必须很简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing but a disgusting materialistic pageant of glistening abed remote control greed zombies totally devoid of any heart or heat a romantic comedy that has zero romantic chemestry and zero laughs\n"
     ]
    }
   ],
   "source": [
    "test_words = review_to_words(test_review)\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们需要构建 `test_words` 字符串的词袋嵌入。词袋嵌入使用了 `vocabulary`，其中包含一组文档中最常出现的字词。然后，对于词汇表中的每个字词，我们都记录该字词出现在 `test_words` 中的次数。我们之前使用训练集构建了 `vocabulary`，所以创建 `test_words` 的词袋编码相对比较简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_encoding(words, vocabulary):\n",
    "    bow = [0] * len(vocabulary) # Start by setting the count for each word in the vocabulary to zero.\n",
    "    for word in words.split():  # For each word in the string\n",
    "        if word in vocabulary:  # If the word is one that occurs in the vocabulary, increase its count.\n",
    "            bow[vocabulary[word]] += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = bow_encoding(test_words, vocabulary)\n",
    "print(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们知道如何为提供的影评创建词袋嵌入，如何将其发送给端点呢？首先，我们需要启动端点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此刻，我们可以像之前测试部署的模型并使用 `xgb_predictor` 对象将 `test_bow` 发送给端点一样操作。但是，当我们最终构建 Lambda 函数后，我们将无法访问此对象，所以如何调用 SageMaker 端点？\n",
    "\n",
    "实际上，Lambda 中使用的 Python 函数能够访问另一个 Amazon 库，叫做 `boto3`。此库提供了使用 Amazon 服务的 API，包括 SageMaker。首先，我们需要获得 SageMaker 运行时的句柄。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "runtime = boto3.Session().client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够访问 SageMaker 运行时后，我们可以要求它使用（调用）已经创建的端点。但是，我们需要告诉 SageMaker 已部署端点的名称。我们可以使用 `xgb_predictor` 对象输出端点名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-2020-03-26-12-52-13-753'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用运行时调用端点并传入端点名称，然后向其发送 `test_bow` 数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName = xgb_predictor.endpoint, # The name of the endpoint we created\n",
    "                                       ContentType = 'text/csv',                     # The data format that is expected\n",
    "                                       Body = test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为何出错了？\n",
    "\n",
    "因为我们向端点发送了整数列表，但是它要求我们发送 `text/csv` 格式的数据，所以需要转换数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName = xgb_predictor.endpoint, # The name of the endpoint we created\n",
    "                                       ContentType = 'text/csv',                     # The data format that is expected\n",
    "                                       Body = ','.join([str(val) for val in test_bow]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '1f9dd197-d51f-4ab2-9a58-bf3a342697ad', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1f9dd197-d51f-4ab2-9a58-bf3a342697ad', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Thu, 26 Mar 2020 13:19:35 GMT', 'content-type': 'text/csv; charset=utf-8', 'content-length': '14'}, 'RetryAttempts': 0}, 'ContentType': 'text/csv; charset=utf-8', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f256af1db70>}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，模型的响应是看起来很复杂的字典，其中包含大量信息。我们最关心的是 `'Body'` 对象，它是一个 streaming 对象，我们需要 `read` Body 以便使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.451680094004\n"
     ]
    }
   ],
   "source": [
    "response = response['Body'].read().decode('utf-8')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知道如何处理用户输入数据后，我们可以开始设置基础设施，使网络应用能运行。我们将使用两个不同的服务，分别是 Amazon 的 Lambda 和 API Gateway 服务。\n",
    "\n",
    "Lambda 服务使用户能够编写相对简单的代码，并且每当所选的触发器被触发时，代码就会执行。例如，每当新数据上传到 S3 上的某个文件夹时，你希望更新数据库。\n",
    "\n",
    "API Gateway 服务使你能够创建 HTTP 端点（网址），该端点与其他 AWS 服务相连。好处之一是你可以决定要访问这些端点，需要具备哪些凭证（如果需要）。\n",
    "\n",
    "我们将通过 API Gateway 设置一个可以公开访问的 HTTP 端点。然后，每当有人向该端点发送数据，我们将触发 Lambda 函数，该函数会将输入（影评）发送给模型的端点，然后返回结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置 Lambda 函数\n",
    "\n",
    "首先设置 Lambda 函数。每当公共 API 向其发送数据，该 Lambda 函数就会执行。当它被执行时，它将接收数据，对数据执行必要的处理步骤，将数据（影评）发送给我们创建的 SageMaker 端点，然后返回结果。\n",
    "\n",
    "#### 第一部分：为 Lambda 函数创建 IAM 角色\n",
    "\n",
    "因为我们希望 Lambda 函数调用 SageMaker 端点，所以需要确保它有权限这么做。我们将创建一个之后分配给 Lambda 函数的角色。\n",
    "\n",
    "在 AWS 控制台中转到 **IAM** 页面并点击 **Roles**，然后点击 **Create role**。确保 **AWS service** 属于所选的受信实体类型，并选择 **Lambda** 作为将使用该角色的服务，然后点击 **Next: Permissions**。\n",
    "\n",
    "在搜索框中输入 `sagemaker` 并选中 **AmazonSageMakerFullAccess** 策略旁边的复选框，然后点击 **Next: Review**。\n",
    "\n",
    "最后命名此角色。设定一个后面能记住的名称，例如 `LambdaSageMakerRole`。然后点击 **Create role**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二部分：创建 Lambda 函数\n",
    "\n",
    "下面开始真正地创建 Lambda 函数。之前提到，为了处理用户提供的输入并将其发送给端点，我们需要以下两项信息：\n",
    "\n",
    " - 端点名称，以及\n",
    " - vocabulary 对象。\n",
    "\n",
    "创建 Lambda 函数后，我们将这些信息复制到 Lambda 函数中。\n",
    "\n",
    "首先在 AWS 控制台中转到 AWS Lambda 页面并点击 **Create a function**。在下个页面选择 **Author from scratch**。下面命名 Lambda 函数，使用后面能记住的名称，例如 `sentiment_analysis_xgboost_func`。记得选中 **Python 3.6** 运行时，然后选择在上个部分创建的角色。接着点击 **Create Function**。\n",
    "\n",
    "在下个页面，你将看到关于刚刚创建的 Lambda 函数的一些信息。向下滚动应该会看到一个编辑器，你可以在其中编写一些代码，当 Lambda 函数被触发时，这些代码将执行。使用在上面编写的处理单条影评的代码，并添加到提供的示例 `lambda_handler` 中，代码如下所示。\n",
    "\n",
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "# And we need the regular expression library to do some of the data processing\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def review_to_words(review):\n",
    "    words = REPLACE_NO_SPACE.sub(\"\", review.lower())\n",
    "    words = REPLACE_WITH_SPACE.sub(\" \", words)\n",
    "    return words\n",
    "    \n",
    "def bow_encoding(words, vocabulary):\n",
    "    bow = [0] * len(vocabulary) # Start by setting the count for each word in the vocabulary to zero.\n",
    "    for word in words.split():  # For each word in the string\n",
    "        if word in vocabulary:  # If the word is one that occurs in the vocabulary, increase its count.\n",
    "            bow[vocabulary[word]] += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    vocab = \"*** ACTUAL VOCABULARY GOES HERE ***\"\n",
    "    \n",
    "    words = review_to_words(event['body'])\n",
    "    bow = bow_encoding(words, vocab)\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '***ENDPOINT NAME HERE***',# The name of the endpoint we created\n",
    "                                       ContentType = 'text/csv',                 # The data format that is expected\n",
    "                                       Body = ','.join([str(val) for val in bow]).encode('utf-8')) # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Round the result so that our web app only gets '1' or '0' as a response.\n",
    "    result = round(float(result))\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : str(result)\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述代码复制粘贴到 Lambda 代码编辑器中后，将 `**ENDPOINT NAME HERE**` 部分替换成我们之前部署的端点的名称。你可以使用以下代码单元格获得端点的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-2020-03-26-12-52-13-753'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，需要将 vocabulary 字典复制到代码中 `lambda_handler` 方法的开头。以下单元格以易于复制粘贴的格式输出了 vocabulary 字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将端点名称添加到 Lambda 函数中后，点击 **Save**。Lambda 函数现在已经在运行。接下来，我们需要让网络应用能够执行 Lambda 函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置 API Gateway\n",
    "\n",
    "Lambda 函数现在已设置好，下面使用 API Gateway 创建新的 API，它将触发我们刚刚创建的 Lambda 函数。\n",
    "\n",
    "在 AWS 控制台中转到 **Amazon API Gateway**，然后点击 **Get started**。\n",
    "\n",
    "在下个页面选中 **New API** 并命名新的 API，例如 `sentiment_analysis_web_app`。然后，点击 **Create API**。\n",
    "\n",
    "我们已经创建了一个 API，但是它目前不能执行任何操作。我们希望它能触发之前创建的 Lambda 函数。\n",
    "\n",
    "选择 **Actions** 下拉菜单并点击 **Create Method**。新的空方法将创建，打开下拉菜单并选择 **POST**，然后选中它旁边的复选框。\n",
    "\n",
    "对于交互选项来说，选择 **Lambda Function** 并点击 **Use Lambda Proxy integration**。这个选项会让发送给 API 的数据直接发送给 Lambda 函数，不做任何处理。它还需要返回值必须是正确的响应对象，因为 API Gateway 也不会进行处理。\n",
    "\n",
    "在 **Lambda Function** 文本框中输入之前创建的 Lambda 函数的名称，然后点击 **Save**。在出现的弹出式方框中点击 **OK**，使 API Gateway 有权调用你创建的 Lambda 函数。\n",
    "\n",
    "创建 API Gateway 的最后一步是打开 **Actions** 下拉菜单并点击 **Deploy API**。你需要创建新的部署阶段并随意命名，例如 `prod`。\n",
    "\n",
    "现在已经成功地设置了能访问 SageMaker 模型的公共 API。记得复制或写下调用新创建的公共 API 所需的 URL，因为在下一步将用到该 URL。你可以在页面顶部找到该 URL，它位于文字 **Invoke URL** 旁边，用蓝色标出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 7 步：部署网络应用\n",
    "\n",
    "创建了可以公开访问的 API 后，我们可以在网络应用中使用它了。我们提供了简单的静态 HTML 文件，该文件可以使用你刚刚创建的公共 API。\n",
    "\n",
    "在 `website` 文件夹里有一个文件 `index.html`。请将该文件下载到计算机上，并在任意文本编辑器中打开该文件。请将 **\\*\\*REPLACE WITH PUBLIC API URL\\*\\*** 替换成在上一步写下的 URL，然后保存文件。\n",
    "\n",
    "现在在本地计算机上打开 `index.html`，浏览器将充当本地网络服务器，你可以使用提供的网站与 SageMaker 模型互动。\n",
    "\n",
    "你还可以将此 HTML 文件托管到任何地方，例如 github，或将静态网站托管到 Amazon 的 S3 上。然后就可以将链接分享给任何人，让他们也能尝试该网站。\n",
    "\n",
    "> **重要事项**：为了使网络应用能与 SageMaker 端点通信，你必须部署和运行端点。这样的话，就要付费了。当你想要使用网络应用的时候，请运行端点，但是不需要使用的时候，要关闭端点，否则会产生巨额 AWS 费用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除端点\n",
    "\n",
    "如果不使用端点，一定要关闭端点。端点按照运行时长计费，所以如果忘记关闭了，可能会产生高昂的费用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可选步骤：清理数据\n",
    "\n",
    "SageMaker 上的默认 notebook 实例没有太多的可用磁盘空间。当你继续完成和执行 notebook 时，最终会耗尽磁盘空间，导致难以诊断的错误。完全使用完 notebook 后，建议删除创建的文件。你可以从终端或 notebook hub 删除文件。以下单元格中包含了从 notebook 内清理文件的命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data_dir directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# And then we delete the directory itself\n",
    "!rmdir $data_dir\n",
    "\n",
    "# Similarly we remove the files in the cache_dir directory and the directory itself\n",
    "!rm $cache_dir/*\n",
    "!rmdir $cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
