{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析\n",
    "\n",
    "## 在 SageMaker 中使用 XGBoost\n",
    "\n",
    "_机器学习工程师纳米学位课程 | 开发_\n",
    "\n",
    "---\n",
    "\n",
    "在这个 Amazon SageMaker 服务使用示例中，我们将构建一个预测影评情感的随机树模型。你可能在上节课已经见过这个示例的一个版本，不过当时是用 sklearn 软件包实现的，现在我们将使用 Amazon 提供的 XGBoost 软件包。\n",
    "\n",
    "## 说明\n",
    "\n",
    "我们已经提供了一些模板代码，但是你需要实现其他功能，才能成功地完成此 notebook。除了要求的部分之外，不需要修改所包含的代码。标题以“**TODO**”开头的部分表示你需要完成或实现其中的某些部分。我们将在每个部分提供说明，并在代码块中用 `# TODO: ...` 注释标记出具体的实现要求。请务必仔细阅读说明。\n",
    "\n",
    "除了实现代码之外，你还需要回答一些问题，这些问题与任务和你的实现代码有关。每个部分需要回答的问题都在标题中以“**问题：**”开头。请仔细阅读每个问题，并编辑下面以“**答案：**”开头的标记单元格，然后输入答案。\n",
    "\n",
    "> 注意：可以通过 **Shift+Enter** 键盘快捷键执行代码和标记单元格。此外，通常还可通过点击单元格（标记单元格需要双击）编辑单元格，或者在选中后按下 **Enter** 键编辑单元格。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 1 步：下载数据\n",
    "\n",
    "我们要使用的数据集很受自然语言处理领域的研究者欢迎，通常称为 [IMDB 数据集](http://ai.stanford.edu/~amaas/data/sentiment/)。其中包含网站 [imdb.com](http://www.imdb.com/) 上的影评，每条影评都标有“**pos**itive”，表示评论者喜欢影片，否则标有“**neg**ative”。\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011.\n",
    "\n",
    "我们先通过 Jupyter Notebook 功能下载和提取该数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2020-03-28 03:54:11--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  52.4MB/s    in 1.5s    \n",
      "\n",
      "2020-03-28 03:54:13 (52.4 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 2 步：准备和处理数据\n",
    "\n",
    "我们下载的文件拆分成了各种文件，每个都包含一条影评。我们需要将这些文件合并成两个文件，一个用于训练，一个用于测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I guess you have to give some points for the sheer courage of writing a musical around a history lesson but how about some decent music? <br /><br />Is the cartoonish acting of Howard DeSilva meant to pique the interest of otherwise jaded children? <br /><br />Is William Daniels\\' campy contemporary (for the time) acting style meant to appeal to a 1960s/70s demographic? <br /><br />Do we need all the \"in-jokes\" about NY & NJ? (I can hear the blue-haired Broadway audience guffawing on cue.) <br /><br />Sorry, I find the whole piece dated, boring & the acting far too strident for the screen'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 3 步：处理数据\n",
    "\n",
    "合并并准备好训练和测试数据集后，我们需要将原始数据处理成机器学习算法能够使用的格式。首先，删除所有的 HTML 格式标记，并执行一些标准自然语言处理步骤，使数据变得类同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取词袋特征\n",
    "\n",
    "对于我们要实现的模型，我们并不直接使用影评，而是将每条影评转换成词袋特征表示法。注意，我们只能访问训练集，所以转换器只能使用训练集创建表示结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
    "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to cache file: bow_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 4 步：使用 XGBoost 进行分类\n",
    "\n",
    "创建了训练（和测试）数据的特征表示结果后，我们将开始设置和使用 SageMaker 提供的 XGBoost 分类器。\n",
    "\n",
    "### 写入数据集\n",
    "\n",
    "我们将使用的 XGBoost 分类器要求我们将数据集写入文件中并将文件存储到 Amazon S3 上。我们首先将训练数据集拆分成两部分，分别是训练集和验证集。然后，将这些数据集写入文件中，并将文件上传到 S3。此外，我们将测试集输入写入文件中并将文件上传到 S3。这样才能使用 SageMaker 批转换功能测试拟合后的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])\n",
    "\n",
    "test_y = pd.DataFrame(test_y)\n",
    "test_X = pd.DataFrame(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker 中的 XGBoost 算法的参考文档要求训练集和验证集不包含标题或索引，并且每个样本的标签在前面。\n",
    "\n",
    "要详细了解此算法以及其他算法，请参阅 [Amazon SageMaker 开发人员文档](https://docs.aws.amazon.com/sagemaker/latest/dg/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make sure that the local directory in which we'd like to store the training and validation csv files exists.\n",
    "data_dir = '../data/xgboost'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All objects passed were None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8563e7b14a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All objects passed were None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# consolidate data & figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All objects passed were None"
     ]
    }
   ],
   "source": [
    "# First, save the test data to test.csv in the data_dir directory. Note that we do not save the associated ground truth\n",
    "# labels, instead we will use them later to compare with our model output.\n",
    "\n",
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a bit of memory we can set text_X, train_X, val_X, train_y and val_y to None.\n",
    "\n",
    "train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) 将训练/验证文件上传到 S3\n",
    "\n",
    "Amazon S3 服务允许我们存储文件，内置训练模型（例如我们将使用的 XGBoost 模型）和自定义模型（例如我们稍后将查看的模型）可以访问这些文件。\n",
    "\n",
    "对于此任务以及将使用 SageMaker 完成的大多数其他任务，我们可以使用两种方法。一种是使用 SageMaker 的低阶方法，低阶方法要求我们知道在 SageMaker 环境中出现的每个对象。第二种是使用高阶方法，SageMaker 会代替我们做出一些选择。低阶方法的好处是给用户带来了很高的灵活性，而高阶方法使开发速度快多了。对我们来说，我们将使用高阶方法，但是也可以使用低阶方法。\n",
    "\n",
    "方法 `upload_data()` 是代表当前 SageMaker 会话的对象的成员。该方法会将数据上传到默认存储桶（如果不存在的话，将会创建），并放入由 key_prefix 变量指定的路径下。上传数据文件后，你可以转到 S3 控制台并看看文件上传到哪了。\n",
    "\n",
    "要查看其他资源，请参阅 [SageMaker API 文档](http://sagemaker.readthedocs.io/en/latest/)以及 [SageMaker 开发人员指南](https://docs.aws.amazon.com/sagemaker/latest/dg/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = 'sentiment-xgboost'\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) 创建优化超参数的 XGBoost 模型\n",
    "\n",
    "上传数据后，下面开始创建 XGBoost 模型。与 Boston Housing notebook 一样，第一步是创建一个 estimator 对象，它将用作超参数优化作业的基础模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Our current execution role is require when creating the model as the training\n",
    "# and inference code will need to access the model artifacts.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='0.90-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    }
   ],
   "source": [
    "# We need to retrieve the location of the container which is provided by Amazon for using XGBoost.\n",
    "# As a matter of convenience, the training and inference code both use the same container.\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a SageMaker estimator using the container location determined in the previous cell.\n",
    "#       It is recommended that you use a single training instance of type ml.m4.xlarge. It is also\n",
    "#       recommended that you use 's3://{}/{}/output'.format(session.default_bucket(), prefix) as the\n",
    "#       output path.\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role,\n",
    "                                    train_instance_count=1,\n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "# TODO: Set the XGBoost hyperparameters in the xgb object. Don't forget that in this case we have a binary\n",
    "#       label so we should be using the 'binary:logistic' objective.\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) 创建超参数优化器\n",
    "\n",
    "设置好基础评估器后，我们需要构建超参数优化器对象，然后要求 SageMaker 使用该对象构建一个超参数优化作业。\n",
    "\n",
    "**注意：**训练一个情感分析 XGBoost 模型的时间比训练 Boston Housing XGBoost 模型的时间要长，所以如果你不希望超参数优化作业的运行时间太长，请不要将模型（作业）总数设得太高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure to import the relevant objects used to construct the tuner\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# TODO: Create the hyperparameter tuner object\n",
    "\n",
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator=xgb,\n",
    "                                               objective_metric_name='validation:logloss',\n",
    "                                               objective_type='Minimize',\n",
    "                                               max_jobs=20,\n",
    "                                               max_parallel_jobs=4,\n",
    "                                               hyperparameter_ranges={\n",
    "                                                   'max_depth': IntegerParameter(3, 12),\n",
    "                                                   'eta'      : ContinuousParameter(0.05, 0.5),\n",
    "                                                   'min_child_weight': IntegerParameter(2, 8),\n",
    "                                                   'subsample': ContinuousParameter(0.5, 0.9),\n",
    "                                                   'gamma'    : ContinuousParameter(0, 10),\n",
    "                                               })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合超参数优化器\n",
    "\n",
    "构建了超参数优化器对象后，下面拟合各种模型并找到性能最佳的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，优化作业已构建并在后台运行，如果你想查看训练作业的进度，请调用 `wait()` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "xgb_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) 测试模型\n",
    "\n",
    "运行超参数优化作业后，下面看看最佳模型的实际效果如何。我们将使用 SageMaker 的批转换功能。通过批转换功能可以轻松地对大型数据集进行推理，因为它并非实时执行。我们并不需要立即使用模型的结果，可以对大量样本进行推理。示例行业应用包括月末报告。这种推理方法的另一个用处是可以对整个测试集进行推理。\n",
    "\n",
    "为了创建执行批转换作业的 transformer 对象，我们需要一个训练过的 estimator 对象。我们可以使用 `attach()` 方法将创建的 estimator 对象连接到最佳训练作业上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-200328-0403-018-893e3abe'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_hyperparameter_tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-28 05:01:29 Starting - Preparing the instances for training\n",
      "2020-03-28 05:01:29 Downloading - Downloading input data\n",
      "2020-03-28 05:01:29 Training - Training image download completed. Training in progress.\n",
      "2020-03-28 05:01:29 Uploading - Uploading generated training model\n",
      "2020-03-28 05:01:29 Completed - Training job completed\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-03-28:04:50:25:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-03-28:04:50:25:INFO] Setting up HPO optimized metric to be : logloss\u001b[0m\n",
      "\u001b[34m[2020-03-28:04:50:25:INFO] File size need to be processed in the node: 238.47mb. Available memory size in the node: 8500.4mb\u001b[0m\n",
      "\u001b[34m[2020-03-28:04:50:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[04:50:25] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[04:50:27] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-03-28:04:50:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[04:50:27] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[04:50:28] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[04:50:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 152 extra nodes, 96 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.653051#011validation-logloss:0.654909\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-logloss' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-logloss hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[04:50:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 164 extra nodes, 76 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.620893#011validation-logloss:0.626424\u001b[0m\n",
      "\u001b[34m[04:50:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 98 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.593471#011validation-logloss:0.602729\u001b[0m\n",
      "\u001b[34m[04:50:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 142 extra nodes, 84 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.571184#011validation-logloss:0.58339\u001b[0m\n",
      "\u001b[34m[04:50:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 122 extra nodes, 84 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.552433#011validation-logloss:0.566945\u001b[0m\n",
      "\u001b[34m[04:50:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 158 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.535176#011validation-logloss:0.552902\u001b[0m\n",
      "\u001b[34m[04:50:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 124 extra nodes, 106 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.520309#011validation-logloss:0.540317\u001b[0m\n",
      "\u001b[34m[04:50:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 116 extra nodes, 80 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.506712#011validation-logloss:0.528614\u001b[0m\n",
      "\u001b[34m[04:51:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 106 extra nodes, 76 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.494405#011validation-logloss:0.518267\u001b[0m\n",
      "\u001b[34m[04:51:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 144 extra nodes, 118 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.48244#011validation-logloss:0.509142\u001b[0m\n",
      "\u001b[34m[04:51:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 152 extra nodes, 94 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.471554#011validation-logloss:0.501122\u001b[0m\n",
      "\u001b[34m[04:51:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 104 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.462002#011validation-logloss:0.493113\u001b[0m\n",
      "\u001b[34m[04:51:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 126 extra nodes, 104 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.452625#011validation-logloss:0.485277\u001b[0m\n",
      "\u001b[34m[04:51:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 78 extra nodes, 60 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.444362#011validation-logloss:0.478506\u001b[0m\n",
      "\u001b[34m[04:51:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 100 extra nodes, 122 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.43646#011validation-logloss:0.472385\u001b[0m\n",
      "\u001b[34m[04:51:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 92 extra nodes, 110 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.429305#011validation-logloss:0.466741\u001b[0m\n",
      "\u001b[34m[04:51:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 108 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.421998#011validation-logloss:0.461057\u001b[0m\n",
      "\u001b[34m[04:51:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 68 extra nodes, 96 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.416236#011validation-logloss:0.45601\u001b[0m\n",
      "\u001b[34m[04:51:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 96 extra nodes, 86 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.409993#011validation-logloss:0.450762\u001b[0m\n",
      "\u001b[34m[04:51:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 88 extra nodes, 76 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.403796#011validation-logloss:0.446267\u001b[0m\n",
      "\u001b[34m[04:51:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 80 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.398889#011validation-logloss:0.442365\u001b[0m\n",
      "\u001b[34m[04:51:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 66 extra nodes, 90 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.393825#011validation-logloss:0.437958\u001b[0m\n",
      "\u001b[34m[04:51:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 74 extra nodes, 70 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.389096#011validation-logloss:0.434102\u001b[0m\n",
      "\u001b[34m[04:51:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 84 extra nodes, 92 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.383863#011validation-logloss:0.43064\u001b[0m\n",
      "\u001b[34m[04:51:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 70 extra nodes, 74 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.378994#011validation-logloss:0.426929\u001b[0m\n",
      "\u001b[34m[04:51:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 68 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.374813#011validation-logloss:0.423869\u001b[0m\n",
      "\u001b[34m[04:51:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 72 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.37065#011validation-logloss:0.420745\u001b[0m\n",
      "\u001b[34m[04:52:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 70 extra nodes, 92 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.366426#011validation-logloss:0.417942\u001b[0m\n",
      "\u001b[34m[04:52:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 74 extra nodes, 102 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.362302#011validation-logloss:0.414436\u001b[0m\n",
      "\u001b[34m[04:52:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 84 extra nodes, 102 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.358115#011validation-logloss:0.411505\u001b[0m\n",
      "\u001b[34m[04:52:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 70 extra nodes, 106 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.35429#011validation-logloss:0.408968\u001b[0m\n",
      "\u001b[34m[04:52:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 74 extra nodes, 82 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.350788#011validation-logloss:0.406448\u001b[0m\n",
      "\u001b[34m[04:52:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 84 extra nodes, 100 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.346596#011validation-logloss:0.403875\u001b[0m\n",
      "\u001b[34m[04:52:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 80 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.343594#011validation-logloss:0.401624\u001b[0m\n",
      "\u001b[34m[04:52:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 82 extra nodes, 84 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.339924#011validation-logloss:0.399273\u001b[0m\n",
      "\u001b[34m[04:52:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 102 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.337271#011validation-logloss:0.397014\u001b[0m\n",
      "\u001b[34m[04:52:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 74 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.334311#011validation-logloss:0.394677\u001b[0m\n",
      "\u001b[34m[04:52:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 70 extra nodes, 88 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.331207#011validation-logloss:0.392876\u001b[0m\n",
      "\u001b[34m[04:52:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 122 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.32865#011validation-logloss:0.390852\u001b[0m\n",
      "\u001b[34m[04:52:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.326074#011validation-logloss:0.388798\u001b[0m\n",
      "\u001b[34m[04:52:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 52 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.323821#011validation-logloss:0.386876\u001b[0m\n",
      "\u001b[34m[04:52:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 64 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.321263#011validation-logloss:0.384854\u001b[0m\n",
      "\u001b[34m[04:52:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 96 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.31887#011validation-logloss:0.3831\u001b[0m\n",
      "\u001b[34m[04:52:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 74 extra nodes, 106 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.315903#011validation-logloss:0.381608\u001b[0m\n",
      "\u001b[34m[04:52:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 86 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.313532#011validation-logloss:0.38069\u001b[0m\n",
      "\u001b[34m[04:53:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 90 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.31099#011validation-logloss:0.379151\u001b[0m\n",
      "\u001b[34m[04:53:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 80 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.308591#011validation-logloss:0.377542\u001b[0m\n",
      "\u001b[34m[04:53:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 102 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.30643#011validation-logloss:0.376556\u001b[0m\n",
      "\u001b[34m[04:53:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 98 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.304308#011validation-logloss:0.375308\u001b[0m\n",
      "\u001b[34m[04:53:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 64 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.302382#011validation-logloss:0.374073\u001b[0m\n",
      "\u001b[34m[04:53:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 80 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[50]#011train-logloss:0.300189#011validation-logloss:0.372588\u001b[0m\n",
      "\u001b[34m[04:53:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[51]#011train-logloss:0.297887#011validation-logloss:0.371277\u001b[0m\n",
      "\u001b[34m[04:53:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[52]#011train-logloss:0.295967#011validation-logloss:0.369531\u001b[0m\n",
      "\u001b[34m[04:53:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 66 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[53]#011train-logloss:0.294235#011validation-logloss:0.368465\u001b[0m\n",
      "\u001b[34m[04:53:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[54]#011train-logloss:0.292239#011validation-logloss:0.367358\u001b[0m\n",
      "\u001b[34m[04:53:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 86 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[55]#011train-logloss:0.290225#011validation-logloss:0.367044\u001b[0m\n",
      "\u001b[34m[04:53:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[56]#011train-logloss:0.288561#011validation-logloss:0.366033\u001b[0m\n",
      "\u001b[34m[04:53:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[57]#011train-logloss:0.286717#011validation-logloss:0.364776\u001b[0m\n",
      "\u001b[34m[04:53:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 96 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[58]#011train-logloss:0.284806#011validation-logloss:0.363339\u001b[0m\n",
      "\u001b[34m[04:53:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 66 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[59]#011train-logloss:0.283008#011validation-logloss:0.362494\u001b[0m\n",
      "\u001b[34m[04:53:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 68 extra nodes, 162 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[60]#011train-logloss:0.281067#011validation-logloss:0.36132\u001b[0m\n",
      "\u001b[34m[04:53:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 146 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[61]#011train-logloss:0.279094#011validation-logloss:0.360201\u001b[0m\n",
      "\u001b[34m[04:53:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 64 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[62]#011train-logloss:0.277387#011validation-logloss:0.35878\u001b[0m\n",
      "\u001b[34m[04:53:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 52 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[63]#011train-logloss:0.276055#011validation-logloss:0.357964\u001b[0m\n",
      "\u001b[34m[04:54:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 82 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[64]#011train-logloss:0.274185#011validation-logloss:0.356865\u001b[0m\n",
      "\u001b[34m[04:54:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 82 extra nodes, 106 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[65]#011train-logloss:0.271974#011validation-logloss:0.355639\u001b[0m\n",
      "\u001b[34m[04:54:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 70 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[66]#011train-logloss:0.270619#011validation-logloss:0.354598\u001b[0m\n",
      "\u001b[34m[04:54:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[67]#011train-logloss:0.269416#011validation-logloss:0.353749\u001b[0m\n",
      "\u001b[34m[04:54:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 68 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[68]#011train-logloss:0.267903#011validation-logloss:0.35263\u001b[0m\n",
      "\u001b[34m[04:54:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 90 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[69]#011train-logloss:0.266489#011validation-logloss:0.351503\u001b[0m\n",
      "\u001b[34m[04:54:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 88 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[70]#011train-logloss:0.265056#011validation-logloss:0.35086\u001b[0m\n",
      "\u001b[34m[04:54:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 70 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[71]#011train-logloss:0.263564#011validation-logloss:0.350182\u001b[0m\n",
      "\u001b[34m[04:54:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 66 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[72]#011train-logloss:0.262432#011validation-logloss:0.349255\u001b[0m\n",
      "\u001b[34m[04:54:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 60 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[73]#011train-logloss:0.26119#011validation-logloss:0.348756\u001b[0m\n",
      "\u001b[34m[04:54:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 46 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[74]#011train-logloss:0.259972#011validation-logloss:0.347695\u001b[0m\n",
      "\u001b[34m[04:54:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 46 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[75]#011train-logloss:0.25892#011validation-logloss:0.34687\u001b[0m\n",
      "\u001b[34m[04:54:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 66 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[76]#011train-logloss:0.257661#011validation-logloss:0.346227\u001b[0m\n",
      "\u001b[34m[04:54:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 64 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[77]#011train-logloss:0.256524#011validation-logloss:0.345357\u001b[0m\n",
      "\u001b[34m[04:54:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 44 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[78]#011train-logloss:0.255255#011validation-logloss:0.344641\u001b[0m\n",
      "\u001b[34m[04:54:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 40 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[79]#011train-logloss:0.254402#011validation-logloss:0.344229\u001b[0m\n",
      "\u001b[34m[04:54:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 68 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[80]#011train-logloss:0.253176#011validation-logloss:0.343576\u001b[0m\n",
      "\u001b[34m[04:54:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 48 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[81]#011train-logloss:0.25216#011validation-logloss:0.342993\u001b[0m\n",
      "\u001b[34m[04:55:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 84 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[82]#011train-logloss:0.250898#011validation-logloss:0.342385\u001b[0m\n",
      "\u001b[34m[04:55:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 114 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[83]#011train-logloss:0.249975#011validation-logloss:0.341749\u001b[0m\n",
      "\u001b[34m[04:55:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 94 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[84]#011train-logloss:0.248898#011validation-logloss:0.341092\u001b[0m\n",
      "\u001b[34m[04:55:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 72 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[85]#011train-logloss:0.24791#011validation-logloss:0.340605\u001b[0m\n",
      "\u001b[34m[04:55:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 62 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[86]#011train-logloss:0.247189#011validation-logloss:0.340019\u001b[0m\n",
      "\u001b[34m[04:55:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[87]#011train-logloss:0.24586#011validation-logloss:0.33937\u001b[0m\n",
      "\u001b[34m[04:55:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 106 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[88]#011train-logloss:0.244874#011validation-logloss:0.338843\u001b[0m\n",
      "\u001b[34m[04:55:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 80 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[89]#011train-logloss:0.243979#011validation-logloss:0.338396\u001b[0m\n",
      "\u001b[34m[04:55:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 60 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[90]#011train-logloss:0.2427#011validation-logloss:0.33792\u001b[0m\n",
      "\u001b[34m[04:55:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 94 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[91]#011train-logloss:0.241685#011validation-logloss:0.337358\u001b[0m\n",
      "\u001b[34m[04:55:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 44 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[92]#011train-logloss:0.240578#011validation-logloss:0.336811\u001b[0m\n",
      "\u001b[34m[04:55:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 92 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[93]#011train-logloss:0.239795#011validation-logloss:0.336255\u001b[0m\n",
      "\u001b[34m[04:55:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 112 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[94]#011train-logloss:0.23872#011validation-logloss:0.335624\u001b[0m\n",
      "\u001b[34m[04:55:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 52 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[95]#011train-logloss:0.237771#011validation-logloss:0.334798\u001b[0m\n",
      "\u001b[34m[04:55:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 50 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[96]#011train-logloss:0.236663#011validation-logloss:0.334372\u001b[0m\n",
      "\u001b[34m[04:55:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 64 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[97]#011train-logloss:0.235833#011validation-logloss:0.334033\u001b[0m\n",
      "\u001b[34m[04:55:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 108 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[98]#011train-logloss:0.234803#011validation-logloss:0.33368\u001b[0m\n",
      "\u001b[34m[04:55:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 60 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[99]#011train-logloss:0.23406#011validation-logloss:0.333253\u001b[0m\n",
      "\u001b[34m[04:56:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 46 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[100]#011train-logloss:0.233209#011validation-logloss:0.332861\u001b[0m\n",
      "\u001b[34m[04:56:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 70 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[101]#011train-logloss:0.232276#011validation-logloss:0.332597\u001b[0m\n",
      "\u001b[34m[04:56:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 52 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[102]#011train-logloss:0.23156#011validation-logloss:0.331963\u001b[0m\n",
      "\u001b[34m[04:56:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 34 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[103]#011train-logloss:0.230945#011validation-logloss:0.331603\u001b[0m\n",
      "\u001b[34m[04:56:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 84 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[104]#011train-logloss:0.229652#011validation-logloss:0.331524\u001b[0m\n",
      "\u001b[34m[04:56:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 86 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[105]#011train-logloss:0.228624#011validation-logloss:0.330744\u001b[0m\n",
      "\u001b[34m[04:56:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 80 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[106]#011train-logloss:0.227844#011validation-logloss:0.330479\u001b[0m\n",
      "\u001b[34m[04:56:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 64 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[107]#011train-logloss:0.227119#011validation-logloss:0.329978\u001b[0m\n",
      "\u001b[34m[04:56:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 40 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[108]#011train-logloss:0.226368#011validation-logloss:0.329733\u001b[0m\n",
      "\u001b[34m[04:56:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 98 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[109]#011train-logloss:0.225329#011validation-logloss:0.329143\u001b[0m\n",
      "\u001b[34m[04:56:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 62 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[110]#011train-logloss:0.224407#011validation-logloss:0.328588\u001b[0m\n",
      "\u001b[34m[04:56:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 70 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[111]#011train-logloss:0.223668#011validation-logloss:0.328046\u001b[0m\n",
      "\u001b[34m[04:56:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 90 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[112]#011train-logloss:0.222785#011validation-logloss:0.327572\u001b[0m\n",
      "\u001b[34m[04:56:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 36 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[113]#011train-logloss:0.222256#011validation-logloss:0.327135\u001b[0m\n",
      "\u001b[34m[04:56:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 48 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[114]#011train-logloss:0.221656#011validation-logloss:0.327047\u001b[0m\n",
      "\u001b[34m[04:56:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 94 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[115]#011train-logloss:0.220986#011validation-logloss:0.326568\u001b[0m\n",
      "\u001b[34m[04:56:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 94 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[116]#011train-logloss:0.220222#011validation-logloss:0.326063\u001b[0m\n",
      "\u001b[34m[04:56:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 48 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[117]#011train-logloss:0.219589#011validation-logloss:0.325685\u001b[0m\n",
      "\u001b[34m[04:56:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[118]#011train-logloss:0.21893#011validation-logloss:0.325446\u001b[0m\n",
      "\u001b[34m[04:57:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 66 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[119]#011train-logloss:0.218415#011validation-logloss:0.325114\u001b[0m\n",
      "\u001b[34m[04:57:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 112 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[120]#011train-logloss:0.217403#011validation-logloss:0.324889\u001b[0m\n",
      "\u001b[34m[04:57:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 42 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[121]#011train-logloss:0.216819#011validation-logloss:0.324517\u001b[0m\n",
      "\u001b[34m[04:57:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 32 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[122]#011train-logloss:0.21622#011validation-logloss:0.324288\u001b[0m\n",
      "\u001b[34m[04:57:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 102 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[123]#011train-logloss:0.215623#011validation-logloss:0.323806\u001b[0m\n",
      "\u001b[34m[04:57:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 56 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[124]#011train-logloss:0.215006#011validation-logloss:0.323797\u001b[0m\n",
      "\u001b[34m[04:57:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 94 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[125]#011train-logloss:0.214496#011validation-logloss:0.323568\u001b[0m\n",
      "\u001b[34m[04:57:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[126]#011train-logloss:0.213997#011validation-logloss:0.32341\u001b[0m\n",
      "\u001b[34m[04:57:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 60 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[127]#011train-logloss:0.21344#011validation-logloss:0.322941\u001b[0m\n",
      "\u001b[34m[04:57:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 56 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[128]#011train-logloss:0.212921#011validation-logloss:0.322738\u001b[0m\n",
      "\u001b[34m[04:57:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[129]#011train-logloss:0.212315#011validation-logloss:0.322594\u001b[0m\n",
      "\u001b[34m[04:57:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 62 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[130]#011train-logloss:0.211847#011validation-logloss:0.3225\u001b[0m\n",
      "\u001b[34m[04:57:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 78 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[131]#011train-logloss:0.211257#011validation-logloss:0.321998\u001b[0m\n",
      "\u001b[34m[04:57:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[132]#011train-logloss:0.210734#011validation-logloss:0.321855\u001b[0m\n",
      "\u001b[34m[04:57:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 34 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[133]#011train-logloss:0.210251#011validation-logloss:0.321632\u001b[0m\n",
      "\u001b[34m[04:57:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 72 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[134]#011train-logloss:0.209719#011validation-logloss:0.321207\u001b[0m\n",
      "\u001b[34m[04:57:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[135]#011train-logloss:0.209197#011validation-logloss:0.321053\u001b[0m\n",
      "\u001b[34m[04:57:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 18 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[136]#011train-logloss:0.208555#011validation-logloss:0.320737\u001b[0m\n",
      "\u001b[34m[04:58:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 66 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[137]#011train-logloss:0.207965#011validation-logloss:0.320498\u001b[0m\n",
      "\u001b[34m[04:58:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 70 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[138]#011train-logloss:0.207421#011validation-logloss:0.320276\u001b[0m\n",
      "\u001b[34m[04:58:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 92 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[139]#011train-logloss:0.206967#011validation-logloss:0.319836\u001b[0m\n",
      "\u001b[34m[04:58:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 68 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[140]#011train-logloss:0.206441#011validation-logloss:0.319672\u001b[0m\n",
      "\u001b[34m[04:58:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 102 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[141]#011train-logloss:0.205561#011validation-logloss:0.319634\u001b[0m\n",
      "\u001b[34m[04:58:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[142]#011train-logloss:0.205137#011validation-logloss:0.319453\u001b[0m\n",
      "\u001b[34m[04:58:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 26 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[143]#011train-logloss:0.204616#011validation-logloss:0.319149\u001b[0m\n",
      "\u001b[34m[04:58:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 94 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[144]#011train-logloss:0.203932#011validation-logloss:0.319056\u001b[0m\n",
      "\u001b[34m[04:58:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 60 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[145]#011train-logloss:0.203581#011validation-logloss:0.31892\u001b[0m\n",
      "\u001b[34m[04:58:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 60 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[146]#011train-logloss:0.202895#011validation-logloss:0.318796\u001b[0m\n",
      "\u001b[34m[04:58:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 52 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[147]#011train-logloss:0.2024#011validation-logloss:0.318231\u001b[0m\n",
      "\u001b[34m[04:58:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 114 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[148]#011train-logloss:0.201739#011validation-logloss:0.318071\u001b[0m\n",
      "\u001b[34m[04:58:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 34 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[149]#011train-logloss:0.201327#011validation-logloss:0.31786\u001b[0m\n",
      "\u001b[34m[04:58:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 74 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[150]#011train-logloss:0.200644#011validation-logloss:0.317699\u001b[0m\n",
      "\u001b[34m[04:58:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 28 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[151]#011train-logloss:0.200074#011validation-logloss:0.317278\u001b[0m\n",
      "\u001b[34m[04:58:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[152]#011train-logloss:0.199611#011validation-logloss:0.316996\u001b[0m\n",
      "\u001b[34m[04:58:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 30 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[153]#011train-logloss:0.199246#011validation-logloss:0.31667\u001b[0m\n",
      "\u001b[34m[04:58:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 64 pruned nodes, max_depth=9\u001b[0m\n",
      "\u001b[34m[154]#011train-logloss:0.198859#011validation-logloss:0.316599\u001b[0m\n",
      "\u001b[34m[04:58:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 60 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[155]#011train-logloss:0.198221#011validation-logloss:0.316459\u001b[0m\n",
      "\u001b[34m[04:59:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 92 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[156]#011train-logloss:0.197836#011validation-logloss:0.316227\u001b[0m\n",
      "\u001b[34m[04:59:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 36 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[157]#011train-logloss:0.197423#011validation-logloss:0.315839\u001b[0m\n",
      "\u001b[34m[04:59:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 34 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[158]#011train-logloss:0.196974#011validation-logloss:0.315715\u001b[0m\n",
      "\u001b[34m[04:59:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[159]#011train-logloss:0.196427#011validation-logloss:0.315551\u001b[0m\n",
      "\u001b[34m[04:59:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 94 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[160]#011train-logloss:0.195965#011validation-logloss:0.315562\u001b[0m\n",
      "\u001b[34m[04:59:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 66 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[161]#011train-logloss:0.195442#011validation-logloss:0.315628\u001b[0m\n",
      "\u001b[34m[04:59:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 26 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[162]#011train-logloss:0.194943#011validation-logloss:0.315524\u001b[0m\n",
      "\u001b[34m[04:59:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 96 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[163]#011train-logloss:0.194303#011validation-logloss:0.315235\u001b[0m\n",
      "\u001b[34m[04:59:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 52 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[164]#011train-logloss:0.193946#011validation-logloss:0.315069\u001b[0m\n",
      "\u001b[34m[04:59:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 36 pruned nodes, max_depth=9\u001b[0m\n",
      "\u001b[34m[165]#011train-logloss:0.193644#011validation-logloss:0.314839\u001b[0m\n",
      "\u001b[34m[04:59:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 70 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[166]#011train-logloss:0.193075#011validation-logloss:0.314401\u001b[0m\n",
      "\u001b[34m[04:59:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 38 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[167]#011train-logloss:0.192735#011validation-logloss:0.31428\u001b[0m\n",
      "\u001b[34m[04:59:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 88 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[168]#011train-logloss:0.192159#011validation-logloss:0.314062\u001b[0m\n",
      "\u001b[34m[04:59:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 36 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[169]#011train-logloss:0.191728#011validation-logloss:0.314071\u001b[0m\n",
      "\u001b[34m[04:59:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 56 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[170]#011train-logloss:0.191249#011validation-logloss:0.313816\u001b[0m\n",
      "\u001b[34m[04:59:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 86 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[171]#011train-logloss:0.190636#011validation-logloss:0.31369\u001b[0m\n",
      "\u001b[34m[04:59:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 76 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[172]#011train-logloss:0.190168#011validation-logloss:0.313514\u001b[0m\n",
      "\u001b[34m[04:59:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 54 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[173]#011train-logloss:0.189846#011validation-logloss:0.31301\u001b[0m\n",
      "\u001b[34m[05:00:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 56 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[174]#011train-logloss:0.189217#011validation-logloss:0.312955\u001b[0m\n",
      "\u001b[34m[05:00:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 58 pruned nodes, max_depth=9\u001b[0m\n",
      "\u001b[34m[175]#011train-logloss:0.188831#011validation-logloss:0.31283\u001b[0m\n",
      "\u001b[34m[05:00:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 92 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[176]#011train-logloss:0.188337#011validation-logloss:0.312729\u001b[0m\n",
      "\u001b[34m[05:00:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 54 pruned nodes, max_depth=8\u001b[0m\n",
      "\u001b[34m[177]#011train-logloss:0.18799#011validation-logloss:0.31248\u001b[0m\n",
      "\u001b[34m[05:00:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 40 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[178]#011train-logloss:0.187876#011validation-logloss:0.312382\u001b[0m\n",
      "\u001b[34m[05:00:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 66 pruned nodes, max_depth=9\u001b[0m\n",
      "\u001b[34m[179]#011train-logloss:0.187418#011validation-logloss:0.312525\u001b[0m\n",
      "\u001b[34m[05:00:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[180]#011train-logloss:0.187037#011validation-logloss:0.311999\u001b[0m\n",
      "\u001b[34m[05:00:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 64 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[181]#011train-logloss:0.186726#011validation-logloss:0.312025\u001b[0m\n",
      "\u001b[34m[05:00:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 58 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[182]#011train-logloss:0.18632#011validation-logloss:0.311709\u001b[0m\n",
      "\u001b[34m[05:00:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 58 pruned nodes, max_depth=8\u001b[0m\n",
      "\u001b[34m[183]#011train-logloss:0.186058#011validation-logloss:0.311785\u001b[0m\n",
      "\u001b[34m[05:00:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 30 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[184]#011train-logloss:0.185715#011validation-logloss:0.311658\u001b[0m\n",
      "\u001b[34m[05:00:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 30 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[185]#011train-logloss:0.185423#011validation-logloss:0.311533\u001b[0m\n",
      "\u001b[34m[05:00:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 46 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[186]#011train-logloss:0.185019#011validation-logloss:0.311422\u001b[0m\n",
      "\u001b[34m[05:00:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 46 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[187]#011train-logloss:0.184387#011validation-logloss:0.311427\u001b[0m\n",
      "\u001b[34m[05:00:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 74 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[188]#011train-logloss:0.18387#011validation-logloss:0.311246\u001b[0m\n",
      "\u001b[34m[05:00:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 38 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[189]#011train-logloss:0.183762#011validation-logloss:0.311267\u001b[0m\n",
      "\u001b[34m[05:00:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 54 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[190]#011train-logloss:0.183505#011validation-logloss:0.311272\u001b[0m\n",
      "\u001b[34m[05:00:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 48 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[191]#011train-logloss:0.183206#011validation-logloss:0.311089\u001b[0m\n",
      "\u001b[34m[05:00:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 82 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[192]#011train-logloss:0.182698#011validation-logloss:0.311143\u001b[0m\n",
      "\u001b[34m[05:01:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 62 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[193]#011train-logloss:0.182551#011validation-logloss:0.310989\u001b[0m\n",
      "\u001b[34m[05:01:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 44 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[194]#011train-logloss:0.182241#011validation-logloss:0.310976\u001b[0m\n",
      "\u001b[34m[05:01:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[195]#011train-logloss:0.181904#011validation-logloss:0.310849\u001b[0m\n",
      "\u001b[34m[05:01:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 66 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[196]#011train-logloss:0.181748#011validation-logloss:0.310714\u001b[0m\n",
      "\u001b[34m[05:01:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 26 pruned nodes, max_depth=10\u001b[0m\n",
      "\u001b[34m[197]#011train-logloss:0.18142#011validation-logloss:0.310505\u001b[0m\n",
      "\u001b[34m[05:01:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 68 pruned nodes, max_depth=11\u001b[0m\n",
      "\u001b[34m[198]#011train-logloss:0.18106#011validation-logloss:0.310505\u001b[0m\n",
      "\u001b[34m[05:01:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 66 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[199]#011train-logloss:0.180912#011validation-logloss:0.310381\u001b[0m\n",
      "Training seconds: 701\n",
      "Billable seconds: 701\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a new estimator object attached to the best training job found during hyperparameter tuning\n",
    "\n",
    "xgb_attached = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经将 estimator 对象连接到了正确的训练作业上，下面像之前一样创建 transformer 对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: xgboost-200328-0403-018-893e3abe\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a transformer object from the attached estimator. Using an instance count of 1 and an instance type of ml.m4.xlarge\n",
    "#       should be more than enough.\n",
    "\n",
    "xgb_transformer = xgb_attached.transformer(instance_count=1,instance_type='ml.m4.xlarge')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来执行转换作业。我们需要指定要发送的数据的类型，使 SageMaker 能够在后台正确地序列化数据。我们将向模型提供 csv 数据，所以指定为 `text/csv`。此外，如果我们提供的测试数据太大，无法一次性处理完，我们需要指定文件的拆分方式。因为数据集中的每行就是一个条目，所以我们将按照每行拆分输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Start the transform job. Make sure to specify the content type and the split type of the test data.\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前转换作业已经在运行，不过是在后台运行。因为我们要等待转换作业运行完毕，所以可以使用 `wait()` 方法查看运行进度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-03-28 05:21:55 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-03-28 05:21:55 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-03-28 05:21:55 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-03-28 05:21:55 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[34m[2020-03-28 05:21:55 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:21:55:INFO] Model loaded successfully for worker : 37\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:21:55:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-03-28 05:21:55 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-03-28 05:21:55 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:21:55:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:21:55:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[32m2020-03-28T05:22:28.110:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:31:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:33:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:38:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-03-28:05:22:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-03-28:05:22:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在转换作业已经执行并且结果（每条影评的预测情感）已经保存到 S3 上。因为我们要在本地分析文件，所以通过一个 notebook 功能将文件复制到 `data_dir`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/371.9 KiB (3.6 MiB/s) with 1 file(s) remaining\r",
      "Completed 371.9 KiB/371.9 KiB (5.1 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-1-270372225889/xgboost-200328-0403-018-893e3abe-2020-03-28-05-18-45-375/test.csv.out to ../data/xgboost/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步是读入模型的输出，将输出转换成可用的格式，我们希望情感为 `1`（正面）或 `0`（负面），然后与真实标签进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86652"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可选步骤：清理数据\n",
    "\n",
    "SageMaker 上的默认 notebook 实例没有太多的可用磁盘空间。当你继续完成和执行 notebook 时，最终会耗尽磁盘空间，导致难以诊断的错误。完全使用完 notebook 后，建议删除创建的文件。你可以从终端或 notebook hub 删除文件。以下单元格中包含了从 notebook 内清理文件的命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data_dir directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# And then we delete the directory itself\n",
    "!rmdir $data_dir\n",
    "\n",
    "# Similarly we will remove the files in the cache_dir directory and the directory itself\n",
    "!rm $cache_dir/*\n",
    "!rmdir $cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
